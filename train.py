{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/gabrielepiccoli/transformer.git\nimport sys\nsys.path.append(\"/kaggle/working/transformer\")\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data.dataloader import DataLoader\n\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\nimport dataset\nfrom model import build_transformer\nfrom config import get_config, get_weights_file_path\nfrom pathlib import Path\nimport torch.utils.tensorboard as tensorboard\n\nimport tqdm\n\ndef greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n\n    sos_idx = tokenizer_src.token_to_id(\"[SOS]\")\n    eos_idx = tokenizer_src.token_to_id(\"[EOS]\")\n\n    encoder_output = model.encode(source, source_mask)\n    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n\n    while True:\n        if decoder_input.size(1) >= max_len:\n            break\n\n        decoder_mask = dataset.causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n\n        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n\n        prob = model.project(out[:,-1])\n        _, next_word = torch.max(prob, dim = 1)\n        decoder_input = torch.cat([decoder_input, torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)], dim = 1)\n\n        if next_word == eos_idx:\n            break\n\n    return decoder_input.squeeze(0)\n\ndef run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples = 2):\n    model.eval()\n    count = 0\n\n    source_texts = []\n    expected = []\n    predicted = []\n\n    console_width = 80\n    with torch.no_grad():\n        for batch in validation_ds:\n            count += 1\n            encoder_input = batch[\"encoder_input\"].to(device)\n            encoder_mask = batch[\"encoder_mask\"].to(device)\n\n            #assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n\n            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n\n            source_text = batch[\"src_text\"][0]\n            target_text = batch[\"tgt_text\"][0]\n            \n            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n\n            source_texts.append(source_text)\n            expected.append(target_text)\n            predicted.append(model_out_text)\n\n            print_msg(\"_\"*console_width)\n            print_msg(f\"SOURCE: {source_text}\")\n            print_msg(f\"EXPECTED: {target_text}\")\n            print_msg(f\"PREDICTION: {model_out_text}\")\n\n            if count >= num_examples:\n                break\n    if writer:\n        a = 1\n\n\n\ndef get_all_sentences(ds, lang):\n    for item in ds:\n        yield item[\"translation\"][lang]\n\ndef get_or_build_tokenizer(config, ds, lang):\n    tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))\n\n    if not Path.exists(tokenizer_path):\n        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n        tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordLevelTrainer(\n            special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n            min_frequency=2\n        )\n\n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n    return tokenizer\n\ndef get_ds(config):\n    ds_raw = load_dataset(\"opus_books\", f\"{config['lang_src']}-{config['lang_tgt']}\", split=\"train\")\n\n\n    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])\n\n    train_ds_size = int(0.9 * len(ds_raw))\n    val_ds_size = len(ds_raw) - train_ds_size\n\n    train_ds_raw, val_ds_raw = torch.utils.data.random_split(ds_raw, [train_ds_size, val_ds_size])\n\n    train_ds = dataset.BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config[\"lang_src\"], config[\"lang_tgt\"], config[\"seq_len\"])\n    val_ds = dataset.BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config[\"lang_src\"], config[\"lang_tgt\"], config[\"seq_len\"])\n    \n    max_len_src = 0\n    max_len_tgt = 0\n\n    for item in ds_raw:\n        src_ids = tokenizer_src.encode(item[\"translation\"][config[\"lang_src\"]]).ids\n        tgt_ids = tokenizer_src.encode(item[\"translation\"][config[\"lang_tgt\"]]).ids\n\n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n\n    print(\"Maxes \", max_len_src, max_len_tgt)\n\n    train_dataloader = DataLoader(train_ds, batch_size = config[\"batch_size\"], shuffle = True)\n    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n\n    print(train_dataloader)\n\n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n\ndef get_model(config, vocab_src_len, vocab_tgt_len):\n    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config[\"seq_len\"], config[\"d_model\"])\n    return model\n\ndef train_model(config):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    Path(config[\"model_folder\"]).mkdir(parents=True, exist_ok=True)\n\n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n    writer = tensorboard.SummaryWriter(config[\"experiment_name\"])\n\n    optimizer = torch.optim.Adam(model.parameters(), lr = config[\"lr\"], eps = 1e-9)\n\n    initial_epoch = 0\n    global_step = 0\n\n    if config[\"preload\"]:\n        model_filename = get_weights_file_path(config, config[\"preload\"])\n        state = torch.load(model_filename)\n        initial_epoch = state[\"epoch\"]\n        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n        global_step = state[\"global_step\"]\n\n    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id(\"[PAD]\"), label_smoothing=0.1)\n\n    for epoch in range(initial_epoch, config[\"num_epochs\"]):\n        model.train()\n        batch_iterator = tqdm.tqdm(train_dataloader, desc = f\"Processing epoch {epoch:02d}\")\n\n        for batch in batch_iterator:\n            encoder_input = batch[\"encoder_input\"].to(device)\n            decoder_input = batch[\"decoder_input\"].to(device)\n            encoder_mask = batch[\"encoder_mask\"].to(device)\n            decoder_mask = batch[\"decoder_mask\"].to(device)\n\n            encoder_output = model.encode(encoder_input, encoder_mask)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n            proj_output = model.project(decoder_output)\n\n            label = batch[\"label\"].to(device)\n            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n\n            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n\n            writer.add_scalar(\"train loss\", loss.item(), global_step)\n            writer.flush()\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config[\"seq_len\"], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n\n            global_step +=1\n\n        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n\n        torch.save({\n            \"epoch\" : epoch,\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"global_step\": global_step,\n        }, model_filename)\n\nif __name__ == \"__main__\":\n    config = get_config()\n    train_model(config)","metadata":{"_uuid":"e5f15ac4-599a-466e-aedd-610c7e9785ca","_cell_guid":"194583dc-6bc0-432d-aca7-8a1a3dd385cf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-10-29T13:09:59.003011Z","iopub.execute_input":"2025-10-29T13:09:59.003841Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'transformer' already exists and is not an empty directory.\nUsing device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"en-it/train-00000-of-00001.parquet:   0%|          | 0.00/5.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"724fe78177fe4450bc932806ca673ac7"}},"metadata":{}}],"execution_count":null}]}